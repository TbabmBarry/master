import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import openml as oml
get_ipython().run_line_magic("matplotlib", " inline")


bananas = oml.datasets.get_dataset(1460) # Banana data has OpenML ID 1460
X, y, _, _ = bananas.get_data(target=bananas.default_target_attribute, dataset_format='array');


plt.scatter(X[:,0], X[:,1], c=y,cmap=plt.cm.bwr, marker='.');


# Plotting helpers. Based loosely on https://github.com/amueller/mglearn
def plot_svm_kernel(X, y, title, support_vectors, decision_function, dual_coef=None, show=True):
    """
    Visualizes the SVM model given the various outputs. It plots:
    * All the data point, color coded by class: blue or red
    * The support vectors, indicated by circling the points with a black border. 
      If the dual coefficients are known (only for kernel SVMs) if paints support vectors with high coefficients darker
    * The decision function as a blue-to-red gradient. It is white where the decision function is near 0.
    * The decision boundary as a full line, and the SVM margins (-1 and +1 values) as a dashed line
    
    Attributes:
    X -- The training data
    y -- The correct labels
    title -- The plot title
    support_vectors -- the list of the coordinates of the support vectores
    decision_function - The decision function returned by the SVM
    dual_coef -- The dual coefficients of all the support vectors (not relevant for LinearSVM)
    show -- whether to plot the figure already or not
    """
    # plot the line, the points, and the nearest vectors to the plane
    #plt.figure(fignum, figsize=(5, 5))
    plt.title(title)
    plt.scatter(X[:, 0], X[:, 1], c=y, zorder=10, cmap=plt.cm.bwr, marker='.')
    if dual_coef is not None:
        plt.scatter(support_vectors[:, 0], support_vectors[:, 1], c=dual_coef[0, :],
                    s=70, edgecolors='k', zorder=10, marker='.', cmap=plt.cm.bwr)
    else:
        plt.scatter(support_vectors[:, 0], support_vectors[:, 1], facecolors='none',
                    s=70, edgecolors='k', zorder=10, marker='.', cmap=plt.cm.bwr)
    plt.axis('tight')
    x_min, x_max = -3.5, 3.5
    y_min, y_max = -3.5, 3.5

    XX, YY = np.mgrid[x_min:x_max:300j, y_min:y_max:300j]
    Z = decision_function(np.c_[XX.ravel(), YY.ravel()])

    # Put the result into a color plot
    Z = Z.reshape(XX.shape)
    plt.contour(XX, YY, Z, colors=['k', 'k', 'k'], linestyles=['--', '-', '--'],
                levels=[-1, 0, 1])
    plt.pcolormesh(XX, YY, Z, vmin=-1, vmax=1, cmap=plt.cm.bwr, alpha=0.1)

    plt.xlim(x_min, x_max)
    plt.ylim(y_min, y_max)
    
    plt.xticks(())
    plt.yticks(())

    if show:
        plt.show()
    
def heatmap(values, xlabel, ylabel, xticklabels, yticklabels, cmap=None,
            vmin=None, vmax=None, ax=None, fmt="%0.2f"):
    """
    Visualizes the results of a grid search with two hyperparameters as a heatmap.
    Attributes:
    values -- The test scores
    xlabel -- The name of hyperparameter 1
    ylabel -- The name of hyperparameter 2
    xticklabels -- The values of hyperparameter 1
    yticklabels -- The values of hyperparameter 2
    cmap -- The matplotlib color map
    vmin -- the minimum value
    vmax -- the maximum value
    ax -- The figure axes to plot on
    fmt -- formatting of the score values
    """
    if ax is None:
        ax = plt.gca()
    # plot the mean cross-validation scores
    img = ax.pcolor(values, cmap=cmap, vmin=None, vmax=None)
    img.update_scalarmappable()
    ax.set_xlabel(xlabel, fontsize=10)
    ax.set_ylabel(ylabel, fontsize=10)
    ax.set_xticks(np.arange(len(xticklabels)) + .5)
    ax.set_yticks(np.arange(len(yticklabels)) + .5)
    ax.set_xticklabels(xticklabels)
    ax.set_yticklabels(yticklabels)
    ax.set_aspect(1)
    
    ax.tick_params(axis='y', labelsize=12)
    ax.tick_params(axis='x', labelsize=12, labelrotation=90)

    for p, color, value in zip(img.get_paths(), img.get_facecolors(), img.get_array()):
        x, y = p.vertices[:-2, :].mean(0)
        if np.mean(color[:3]) > 0.5:
            c = 'k'
        else:
            c = 'w'
        ax.text(x, y, fmt % value, color=c, ha="center", va="center", size=10)
    return img


from sklearn.svm import LinearSVC
clf = LinearSVC(C=0.001, loss="hinge").fit(X, y)
support_vector_indices = np.where((2 * y - 1) * clf.decision_function(X) <= 1)[0]
support_vectors = X[support_vector_indices]


support_vectors


plot_svm_kernel(X, y, "Linear SVMs", support_vectors, clf.decision_function, dual_coef=None, show=True)


# Hint: how to compute the support vectors from the decision function (ignore if you want to solve this yourself)
# support_vector_indices = np.where((2 * y - 1) * clf.decision_function(X) <= 1)[0]
# support_vectors = X[support_vector_indices]

# Note that we can also calculate the decision function manually with the formula y = w*X
# decision_function = np.dot(X, clf.coef_[0]) + clf.intercept_[0]


from sklearn.svm import SVC
from sklearn.model_selection import cross_val_score


models = [SVC(kernel="linear"),
         SVC(kernel="rbf"),
         SVC(kernel="poly")]
scores = [cross_val_score(clf, X, y, cv=5) for clf in models]


model_names = ["linear", "RBF", "polynomial"]
for name, score in zip(model_names, scores):
    print(f'{name} returns {score.mean():.3f} accuracy with a standard deviation of {score.var():.3f}')


plt.rcParams['figure.dpi'] = 120
models = [SVC(kernel="linear", C=1.0, tol=1e-3),
         SVC(kernel="rbf", gamma=2, C=1.0),
         SVC(kernel="poly", C=1.0)]
model_names = ["linear", "RBF", "polynomial"]
models = [clf.fit(X, y) for clf in models]
for title, clf in zip(model_names, models):
    plot_svm_kernel(X, y, title, clf.support_vectors_, clf.decision_function, clf.dual_coef_, show=True)


from sklearn.model_selection import train_test_split

# For convenience we'll plot the results in a 3x3 grid
plt.figure(figsize=(15, 10))
fig_num = 0

# build a standard stratified train-test split
X_train, X_test, y_train, y_test = train_test_split(X,y, stratify=y, random_state=0)


C_2d_range = [1e-3, 1, 1e3]
gamma_2d_range = [1e-3, 1, 1e3]
for C_param in C_2d_range:
    for gamma_param in gamma_2d_range:
        fig_num += 1
        plt.subplot(3, 3, fig_num)
        clf = SVC(C=C_param, gamma=gamma_param)
        clf.fit(X_train, y_train)
        plot_svm_kernel(X_train, y_train, f'C: {C_param:.3f}; gamma: {gamma_param:.3f}', clf.support_vectors_, clf.decision_function, clf.dual_coef_, show=False)

plt.show()


from sklearn.model_selection import train_test_split, cross_validate, GridSearchCV, StratifiedShuffleSplit


Xs, _, ys, _ = train_test_split(X, y, stratify=y, train_size=0.1)


C_params = np.logspace(-12, 12, 25, base=2)
gamma_params = np.logspace(-12, 12, 25, base=2)
grid_params = dict(C=C_params, gamma=gamma_params)
cv = StratifiedShuffleSplit(n_splits=3, test_size=0.2, random_state=42)
grid = GridSearchCV(SVC(), param_grid=grid_params, n_jobs=-1, cv=cv, scoring='accuracy')
grid.fit(X, y)


test_scores = np.array(pd.DataFrame(grid.cv_results_).mean_test_score).reshape(25, 25).T
plt.rcParams.update({'font.size': 18})
fig, axes = plt.subplots(1, 1, figsize=(13, 13))
heatmap(test_scores, xlabel='C', xticklabels=np.around(grid_params['C'],4),
        ylabel='gamma', yticklabels=np.around(grid_params["gamma"],4), cmap="viridis", fmt="%.2f", ax=axes)






